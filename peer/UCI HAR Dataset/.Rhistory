head(mtcars)
tail(mtcars)
# We can also find out quick facts about our data with the str() (structure) function
str(mtcars)
# Creating data frames is easy, in this example, we create data based on the 8 planets
# First, we define the vectors
name <- c("Mercury", "Venus", "Earth", "Mars", "Jupiter", "Saturn", "Uranus", "Neptune")
type <- c("Terrestrial planet", "Terrestrial planet", "Terrestrial planet",
"Terrestrial planet", "Gas giant", "Gas giant", "Gas giant", "Gas giant")
diameter <- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883)
rotation <- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67)
rings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)
# Then we put them together into a dataframe utilizing the data.frame() function
planets_df <- data.frame(name, type, diameter, rotation, rings)
planets_df
# We can explore the structure of our dataframe
str(planets_df)
# We can select elements within the dataframe through the use of square brackets,
# my_df[1,2] selects the value and the first row and second column in my_df
# my_df[1:3,2:4] selects rows 1,2,3 and columns 2,3,4 in my_df
# We can select all elements in a row or column by writing my_df[1,] selects all
# elements in row 1
# Select diamater of Mercury
planets_df[1,3]
# Select all Mars data
planets_df[4,]
planets_df[1:3,2]
planets_df[1:3,2]
# or by writing this
planets_df[1:3,"type"]
planets_df[1:5,"diameter"]
planets_df$diameter
# A data frame has the variables of a data set as columns and the observations
# as rows. This will be a familiar concept for those coming from different
# statistical software packages such as SAS or SPSS.
# These vary from matrices as matrices in R allow only one data type while dfs
# allow for multiple data types
# R has built in data frames, such as mtcars
mtcars
# mtcars is a big dataset, and we can use head() to see the top portion and
# tail() to see the bottom portion
head(mtcars)
tail(mtcars)
# We can also find out quick facts about our data with the str() (structure) function
str(mtcars)
# Creating data frames is easy, in this example, we create data based on the 8 planets
# First, we define the vectors
name <- c("Mercury", "Venus", "Earth", "Mars", "Jupiter", "Saturn", "Uranus", "Neptune")
type <- c("Terrestrial planet", "Terrestrial planet", "Terrestrial planet",
"Terrestrial planet", "Gas giant", "Gas giant", "Gas giant", "Gas giant")
diameter <- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883)
rotation <- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67)
rings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)
# Then we put them together into a dataframe utilizing the data.frame() function
planets_df <- data.frame(name, type, diameter, rotation, rings)
planets_df
# We can explore the structure of our dataframe
str(planets_df)
# We can select elements within the dataframe through the use of square brackets,
# my_df[1,2] selects the value and the first row and second column in my_df
# my_df[1:3,2:4] selects rows 1,2,3 and columns 2,3,4 in my_df
# We can select all elements in a row or column by writing my_df[1,] selects all
# elements in row 1
# Select diamater of Mercury
planets_df[1,3]
# Select all Mars data
planets_df[4,]
# We can also use variable names to select columns in a data frame
# We can select the first three elements of the type column in planets_df,
# by writing
planets_df[1:3,2]
# or by writing this
planets_df[1:3,"type"]
# Select the first 5 values in the "diameter" column of planets_df
planets_df[1:5,"diameter"]
# We also have a further shortcut, as shown below
planets_df$diameter
#Select the rings column and name it ring_vector
ring_vector <- planets_df$rings
ring_vector
planets_df[rings_vector,]
planets_df$diameter
#Select the rings column and name it ring_vector
rings_vector <- planets_df$rings
rings_vector
# We can use rings_vector to select only the planets with rings
planets_df[rings_vector,]
subset(planets_df, subset = rings)
rings
subset(planets_df, subset = diameter < 1)
install.packages("KernSmooth")
load(KernSmooth)
library(KernSmooth)
?gl
swirl()
# Programming in R swirl exercises
# Week 1
# Install swirl
install.packages("swirl")
# Load swirl
library(swirl)
# Install R Programming Course
install_from_swirl("R Programming")
# Start swirl
swirl()
head(flags)
dim(flags)
viewinfo()
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[, 11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors, mean)
flag_shapes <- flags[, 19:23]
lapply(flag_shapes, range)
sapply(flag_shapes, range) -> shape_mat
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6)).
unique(c(3, 4, 5, 5, 5, 6, 6))
lapply(flags, unique) -> unique_vals
unique_vals
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
str(flags)
summary(flags)
sapply(flags,unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean )
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmass, summary)
library(datasets)
data("iris")
data(iris)
data("iris")
iris
tapply(iris$Sepal.Length, iris$Species, mean)
round(tapply(iris$Sepal.Length, iris$Species, mean))
head(iris)
colMeans(iris)
apply(iris[, 1:4], 1, mean)
apply(iris, 2, mean)
apply(iris[, 1:4], 2, mean)
data(mtcars)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
tapply(mtcars$mpg, mtcars$cyl, mean)
split(mtcars, mtcars$cyl)
sapply(mtcars, cyl, mean)
tapply(mtcars$cyl, mtcars$mpg, mean)
with(mtcars, tapply(mpg, cyl, mean))
mean(mtcars$mpg, mtcars$cyl)
tapply(mtcars$hp, mtcars$cyl)
tapply(mtcars$hp, mtcars$cyl, mean)
range(tapply(mtcars$hp, mtcars$cyl, mean))
range(tapply(mtcars$hp, mtcars$cyl, mean))
> tapply(mtcars$hp, mtcars$cyl, mean)[3]-> tapply(mtcars$hp, mtcars$cyl, mean)
[1]
> tapply(mtcars$hp, mtcars$cyl, mean)[3]-> tapply(mtcars$hp, mtcars$cyl, mean)[1]
> tapply(mtcars$hp, mtcars$cyl, mean)[3]-tapply(mtcars$hp, mtcars$cyl, mean)[1]
209.21429 - 82.63636
round(209.21429 - 82.63636)
debug(ls)
ls()
3
2
1
tapply(iris$Sepal.Length, iris$Species, mean)
apply(iris[, 1:4], 2, mean)
mtcars
tapply(mtcars$hp, mtcars$cyl, mean)
tapply(mtcars$hp, mtcars$cyl, mean)[3]
tapply(mtcars$hp, mtcars$cyl, mean)[3]-tapply(mtcars$hp, mtcars$cyl, mean)[1]
ls()
getwd()
?<<-
?`<<-``
?`<<-`
?matrix
matrix(rnorm(16), 4,4)
solve(matrix(rnorm(16), 4,4))
matrix(c(2,1,3,4), 2,2)
solve(matrix(c(2,1,3,4), 2,2))
> matrix(c(2,1,3,4), 2,2) %*% > matrix(c(2,1,3,4), 2,2)
> matrix(c(2,1,3,4), 2,2) %*% matrix(c(2,1,3,4), 2,2)
matrix(c(2,1,3,4), 2,2) %*% matrix(c(2,1,3,4), 2,2)
matrix(c(2,1,3,4), 2,2) * matrix(c(2,1,3,4), 2,2)
matrix(c(2,1,3,4), 2,2) * solve(matrix(c(2,1,3,4), 2,2))
matrix(c(2,1,3,4), 2,2) %*% solve(matrix(c(2,1,3,4), 2,2))
matrix(c(2,1,3,4), 100,100) %*% solve(matrix(c(2,1,3,4), 100,100))
matrix(c(2,1,3,4), 100,100)
setinv <- function(solve) m <<- solve
## Put comments here that give an overall description of what your
## functions do
## Write a short comment describing this function
makeCacheMatrix <- function(x = matrix()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setinv <- function(solve) m <<- solve
getinv <- function() m
list(set = set, get = get, setinv = setinv, getinv = getinv)
}
## Write a short comment describing this function
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
m <- x$getinv()
if(!is.null(m)) {
message("Obtaining cached data")
return(m)
}
data <- x$get()
m <- solve(mat, ...)
x$setinv(m)
m
}
c <- matrix(c(1:100), 10, 10)
c
cacheSolve(c)
makeCacheMatrix(c)
test <- makeCacheMatrix(c)
test
cacheSolve(c)
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
m <- getinv(x)
if(!is.null(m)) {
message("Obtaining cached data")
return(m)
}
data <- x$get()
m <- solve(mat, ...)
x$setinv(m)
m
}
cacheSolve(c)
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
m <- x$getinv()
if(!is.null(m)) {
message("Obtaining cached data")
return(m)
}
data <- x$get()
m <- solve(mat, ...)
x$setinv(m)
m
}
cacheSolve(c)
cacheSolve(test)
cacheSolve(test, c)
cacheSolve(test, mat = c)
solve(c)
c <- matrixrnorm(1:100), 10, 10)
c <- matrix(rnorm(1:100), 10, 10)
c
test <- makeCacheMatrix(c)
test
cacheSolve(test, c)
cacheSolve(test, c)
## Put comments here that give an overall description of what your
## functions do
## Write a short comment describing this function
makeCacheMatrix <- function(x = matrix()) {
m <- NULL
set <- function(y) {
x <<- y
m <<- NULL
}
get <- function() x
setinv <- function(solve) m <<- solve
getinv <- function() m
list(set = set, get = get, setinv = setinv, getinv = getinv)
}
## Write a short comment describing this function
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
m <- x$getinv()
if(!is.null(m)) {
message("Obtaining cached data")
return(m)
}
data <- x$get()
m <- solve(...)
x$setinv(m)
m
}
cacheSolve(test, c)
cacheSolve(test, c) -> testinv
testinv
test %*% testinv
c %*% testinv
round(c %*% testinv()
round(c %*% testinv)
c %*% testinv
c %*% testinv -> temp
round(temp)
test
swirl()
library(swirl)
rm(list = ls())
swirl()
swirl()
swirl()
swirl()
select(cran, size:ip_id)
select(cran, size:ip_id) -> cran2
arrange(ip_id, ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
# repo was created.  What time was it created?
install.packages("jsonlite")
library("httr",jsonlite)
oauth_endpoints("github")
myapp <- oauth_app("jfertest",
key = "aa1069dc69d329d8b7fe",
secret = "cdb338543b67c9da798e946a3fe631a644668c43")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
swirl()
read.csv(patch2csv, stringsAsFactors = FALSE) -> mydf
read.csv("patch2csv", stringsAsFactors = FALSE) -> mydf
path2csv
read.csv(patch2csv, stringsAsFactors = FALSE) -> mydf
install.packages(c("acepack", "assertthat", "backports", "BH", "boot", "chron", "class", "cluster", "codetools", "colorspace", "curl", "data.table", "digest", "dplyr", "evaluate", "formatR", "Formula", "ggplot2", "ggvis", "gridExtra", "gtable", "Hmisc", "htmltools", "httpuv", "knitr", "lattice", "lazyeval", "markdown", "MASS", "Matrix", "mgcv", "mime", "mnormt", "nlme", "nnet", "openssl", "plyr", "psych", "R6", "reshape2", "rgl", "rmarkdown", "rpart", "rsconnect", "rstudioapi", "scales", "shiny", "spatial", "stringi", "stringr", "survival", "tidyr", "tidyselect"))
knitr::opts_chunk$set(echo = TRUE)
test <- read.delim("\test\X_test.txt", sep = " ")
test <- read.delim("\test\\X_test.txt", sep = " ")
test <- read.delim("...\test\X_test.txt", sep = " ")
ls()
list.files("\test")
list.files("\test\")
test <- read.delim(".\test\X_test.txt", sep = " ")
test <- read.delim("~\test\X_test.txt", sep = " ")
files1
files1 <- list.files()
files1
unzipped <- files1[2]
setwd(unzipped)
list.files()
setwd(unzipped)
list.files()
files1 <- list.files()
unzipped <- files1[2]
setwd(unzipped)
list.files()
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/week_4/peer/UCI HAR Dataset")
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/peer/UCI HAR Dataset")
files1 <- list.files()
unzipped <- files1[2]
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/peer/UCI HAR Dataset")
list.files()
---
title: "week4_peer"
author: "Fernando Rodriguez"
date: "9/9/2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Getting and Cleaning Data Course Project
The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit:
* A tidy data set as described below,
* a link to a Github repository with your script for performing the analysis, and
* a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called CodeBook.md.
You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected.
One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
Here are the data for the project:
https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
You should create one R script called run_analysis.R that does the following.
* Merges the training and the test sets to create one data set.
* Extracts only the measurements on the mean and standard deviation for each measurement.
* Uses descriptive activity names to name the activities in the data set
* Appropriately labels the data set with descriptive variable names.
* From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
Good luck!
```{r download, echo=TRUE}
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/week_4/peer")
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
file1 <- "dataset.zip"
if(!file.exists(file1)) {
dataset <- download.file(url1, destfile = file1, method = "curl")
}
# Unzip file
unzip(file1, exdir = ".")
files1 <- list.files()
unzipped <- files1[2]
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/peer/UCI HAR Dataset")
train <- read.delim("/train/X_train.txt", sep = " ")
list.files()
```
train
train <- read.delim("./train/X_train.txt", sep = " ")
train <- read.delim("train/X_train.txt", sep = " ")
train <- read.delim("train/X_train.txt", sep = " ")
getwd()
list.files()
list.files("train")
train <- read.delim("test/X_test.txt", sep = " ")
train <- read.csv("test/X_test.txt", sep = " ")
train <- read.csv(testdir, sep = " ")
---
title: "week4_peer"
author: "Fernando Rodriguez"
date: "9/9/2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Getting and Cleaning Data Course Project
The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit:
* A tidy data set as described below,
* a link to a Github repository with your script for performing the analysis, and
* a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called CodeBook.md.
You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected.
One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
Here are the data for the project:
https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
You should create one R script called run_analysis.R that does the following.
* Merges the training and the test sets to create one data set.
* Extracts only the measurements on the mean and standard deviation for each measurement.
* Uses descriptive activity names to name the activities in the data set
* Appropriately labels the data set with descriptive variable names.
* From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
Good luck!
```{r download, echo=TRUE}
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/week_4/peer")
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
file1 <- "dataset.zip"
if(!file.exists(file1)) {
dataset <- download.file(url1, destfile = file1, method = "curl")
}
# Unzip file
unzip(file1, exdir = ".")
files1 <- list.files()
unzipped <- files1[2]
"/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/peer/UCI HAR Dataset/test/X_test.txt" -> testdir
train <- read.csv(testdir, sep = " ")
list.files()
```
---
title: "week4_peer"
author: "Fernando Rodriguez"
date: "9/9/2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Getting and Cleaning Data Course Project
The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit:
* A tidy data set as described below,
* a link to a Github repository with your script for performing the analysis, and
* a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called CodeBook.md.
You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected.
One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
Here are the data for the project:
https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
You should create one R script called run_analysis.R that does the following.
* Merges the training and the test sets to create one data set.
* Extracts only the measurements on the mean and standard deviation for each measurement.
* Uses descriptive activity names to name the activities in the data set
* Appropriately labels the data set with descriptive variable names.
* From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
Good luck!
```{r download, echo=TRUE}
setwd("/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/week_4/peer")
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
file1 <- "dataset.zip"
if(!file.exists(file1)) {
dataset <- download.file(url1, destfile = file1, method = "curl")
}
# Unzip file
unzip(file1, exdir = ".")
files1 <- list.files()
unzipped <- files1[2]
xtest <- "/Users/fer/Dropbox/Coursera DS/Getting_Cleaning_Data/peer/UCI HAR Dataset/test/X_test.txt"
train <- read.delim(xtest, sep = " ")
list.files()
```
train
tbl_df(train)
install.packages("dplyr")
tbl_df(train)
library("dplyr")
tbl_df(train)
train <- read.delim(xtest, sep = " ", header = FALSE)
train <- tbl_df(train)
train
?read.delim
